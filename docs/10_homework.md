# 附录: 课后作业 — Workflow 实战

> 学完了全部章节，是时候自己动手了。这次没有手把手教程，只有需求。
>
> | 章节 | 关键词 |
> |:-----|:------|
> | 工具选型 | 工具选型 · 开发模式 |
> | 需求分析 | Plan Agent · 需求分析 |
> | 搭建脚手架 | CLAUDE.md · CLI 搭建 |
> | 解析 Git Diff | Explore Agent · Git Diff |
> | Agent 设计 | Agent 设计 · Prompt 工程 |
> | Fan-out/Fan-in | Fan-out/Fan-in · 并行执行 |
> | 结果聚合 | 结果聚合 · 条件逻辑 |
> | Hooks 与 Skills | Hooks · Skills |
> | 测试驱动 | 测试策略 · TDD |
> | 六种编排模式 | 模式提炼 · 最佳实践 |
> | **► 附录 课后作业** | **Workflow 实战** |

---

## 作业列表

4 个作业覆盖软件开发生命周期的不同阶段，每个作业侧重不同的模式组合：

| # | 作业 | 场景 | 核心模式 |
|---|------|------|----------|
| 1 | Develop Workflow | 从需求到可运行代码的全流程 | Explore → Specialized Agent → Sequential → Test-Driven |
| 2 | Test Workflow | 自动生成测试套件并驱动质量闭环 | Explore → Fan-out/Fan-in → Test-Driven → Event-Driven |
| 3 | Security Audit Workflow | 扫描项目安全隐患并生成审计报告 | Explore → Specialized Agent → Fan-out/Fan-in → Sequential |
| 4 | Compliance Test Workflow | 检查终端设备/移动 OS 是否符合工信部及网络安全法规 | Explore → Specialized Agent → Fan-out/Fan-in → Event-Driven |

---

## 共同背景

作业 1-3 共享同一个目标项目：一个 C/C++ 网络库，包含 HTTP 协议解析器和 TCP 连接池，约 60 个文件，12000 行代码。作业 4 的目标项目是一个国产移动操作系统的系统级组件（同样是 C/C++ 实现）。

你可以用任何现有的开源项目作为练习对象，也可以自己搭一个最小骨架。

---

## 建议顺序

1. 先做 **Develop Workflow**——最基础的 Sequential 编排，流程固定，帮你热身
2. 再做 **Test Workflow**——加入 Fan-out 和 Test-Driven 循环，需要自己决定测试生成的拆分策略
3. 然后做 **Security Audit Workflow**——纯分析型任务，agent 架构、误报控制、评级体系全部自己设计
4. 最后做 **Compliance Test Workflow**——最综合，需要把法规领域知识映射到 agent 架构，还涉及自动整改

当然，你也可以按自己的兴趣随意挑选。

> 没有标准答案。用你在这本教程中学到的模式，设计你自己的方案。祝你编排愉快。

---

## 作业 1：Develop Workflow

> 构建一个自动化功能开发工具，从需求描述到可运行代码的全流程编排。

**核心模式**：Explore → Specialized Agent → Sequential → Test-Driven

**术语**

- Scaffold（脚手架，自动生成项目骨架代码）
- Code Generation（代码生成，由 AI 根据规格产出源代码）
- Integration Test（集成测试，验证多个模块协同工作的测试）
- Stub（桩，占位实现，用于先搭结构后填逻辑）

### 背景

你的团队维护一个 C/C++ 网络库。产品经理提了一个需求："给连接池加上健康检查功能——定期 ping 池中的空闲连接，剔除已断开的连接。"

手动开发流程：读需求 → 看现有代码 → 设计接口 → 写实现 → 写测试 → 跑测试 → 修 bug。每一步都要人盯着。

你想用 Claude Code 的 workflow 编排，把这个流程自动化成一条命令。

### 需求描述

**输入**:
- 一个本地 C/C++ 项目的路径
- 一段自然语言的功能需求描述

**输出**:
- 新增/修改的源文件（.c/.h）
- 对应的单元测试文件
- 一份变更摘要（改了哪些文件、新增了哪些接口）

**开发阶段**

工具需要按顺序完成以下 4 个阶段：

| 阶段 | 任务 | 对应模式 |
|------|------|---------|
| 理解 | 探索现有代码结构，识别需要修改的模块和接口 | Explore |
| 设计 | 生成接口定义（.h 文件），确定函数签名和数据结构 | Specialized Agent |
| 实现 | 根据接口定义生成实现代码（.c 文件） | Sequential |
| 验证 | 生成测试、编译、运行测试、修复失败 | Test-Driven |

**技术要求**

1. 用 CLI 封装（typer），支持 `dev implement <project-path> --requirement "需求描述"` 命令
2. 理解阶段用 Explore agent 只读探索，不修改任何文件
3. 设计阶段先生成 .h 文件，经用户确认后再进入实现阶段
4. 实现阶段生成的代码必须遵循项目现有的命名规范和代码风格
5. 验证阶段：编译 → 运行测试 → 失败则自动修复 → 重新验证（最多 3 轮）
6. 支持 `--dry-run` 参数，只输出设计方案不写文件

### 提示：你会用到哪些模式？

- [ ] **Explore** — 理解现有代码结构和接口
- [ ] **Specialized Agent** — 接口设计 agent、实现 agent、测试 agent 各司其职
- [ ] **Sequential** — 理解 → 设计 → 实现 → 验证的严格顺序
- [ ] **Test-Driven** — 生成测试 → 编译运行 → 自动修复循环

### 参考架构

```
dev implement <path> --requirement "..."
    │
    ├── 1. Explore: 理解现有代码
    │   └── 输出: 模块依赖图、相关接口、命名规范
    │
    ├── 2. Design: 生成接口定义
    │   └── 输出: 新增的 .h 文件（函数签名 + 数据结构）
    │   └── 等待用户确认
    │
    ├── 3. Implement: 生成实现代码
    │   └── 输出: 新增/修改的 .c 文件
    │
    └── 4. Verify: 测试循环
        ├── 生成测试文件
        ├── 编译全部代码
        ├── 运行测试
        └── 失败 → 自动修复 → 重新编译运行（最多 3 轮）
```

### 验收标准

1. `dev implement ./my-network-lib --requirement "需求描述"` 一条命令完成开发
2. 生成的 .h 文件包含清晰的函数签名和注释
3. 生成的 .c 文件能通过编译（gcc -Wall -Werror）
4. 自动生成的测试覆盖核心逻辑且全部通过
5. 生成的代码风格与项目现有代码一致

### 进阶挑战

- 支持多文件变更：一个需求涉及多个模块时，自动识别所有需要修改的文件
- 生成 git commit message，自动提交变更
- 用 Hook 实现"每次生成代码后自动跑 clang-format"
- 支持 `--lang c++` 参数，生成 C++ 风格的代码（类、RAII、智能指针）

---

## 作业 2：Test Workflow

> 构建一个自动化测试生成工具，为 C/C++ 项目生成测试套件并驱动质量闭环。

**核心模式**：Explore → Fan-out/Fan-in → Test-Driven → Event-Driven

**术语**

- Coverage（覆盖率，测试执行到的代码占总代码的比例）
- gcov（GCC 自带的代码覆盖率工具）
- Boundary Test（边界测试，针对输入边界值的测试用例）
- Fuzz Testing（模糊测试，用随机/半随机数据探测程序崩溃）
- Regression Suite（回归测试套件，防止旧功能被新改动破坏）

### 背景

你的 C/C++ 网络库有 12000 行代码，但测试覆盖率只有 23%。老板说："下个迭代之前，核心模块的覆盖率要到 80%。"

手动写测试太慢，而且容易漏掉边界情况。你决定用 Claude Code 的 workflow 编排，构建一个自动化测试生成工具——它能读懂代码，自动生成测试，跑测试，发现失败后自动修复，直到全部通过。

### 需求描述

**输入**: 一个本地 C/C++ 项目的路径，可选指定模块

**输出**:
- 生成的测试文件（每个源文件对应一个测试文件）
- 测试执行报告（通过/失败/跳过）
- 覆盖率报告（行覆盖率、分支覆盖率）

### 你需要自己决定的事

教程中的 Review Bot 把审查拆成 4 个固定维度并行跑，因为审查维度之间几乎没有依赖。但测试生成不一定适合同样的拆法。

**Q1: 按什么维度拆分测试生成 agent？**

至少有三种拆法：

- **按测试类型拆**（正常路径 / 边界条件 / 错误处理 / 并发安全 / ...）：每个 agent 专注一种测试策略，容易并行。但同一个函数会被多个 agent 重复分析，且 agent 之间可能生成重复的测试用例。
- **按模块拆**（parser / conn_pool / http_handler / ...）：每个 agent 负责一个模块的完整测试，上下文更集中。但不同模块的复杂度差异大，agent 负载不均衡。
- **按覆盖率缺口拆**：先跑一轮基础测试拿到覆盖率数据，再按未覆盖的代码路径分配给不同 agent 定向补充。精准度最高，但变成了两阶段串行。

你选哪种？还是混合使用？

**Q2: 多个 agent 的测试怎么合并？**

如果多个 agent 并行生成测试，合并时会遇到实际问题：
- 重复的测试用例怎么去重？
- 冲突的 mock 或全局状态设置怎么处理？
- 测试文件的组织结构怎么定——按维度分文件，还是按模块分文件？

**Q3: 修复循环的策略是什么？**

"最多 3 轮"是最简单的策略。但你可以设计更聪明的方案：
- 编译失败和运行失败是否应该用不同的修复策略？
- 如果第 2 轮修复引入了新的失败，是继续修还是回退？
- 覆盖率未达标算"失败"吗？要不要触发额外的测试生成？

**技术要求**

1. 用 CLI 封装（typer），支持 `testgen run <project-path>` 命令
2. 可选 `--module <name>` 参数，只为指定模块生成测试
3. agent 的数量和分工由你自己设计——不一定按测试类型拆，不一定全部并行
4. 生成前先用 Explore agent 理解项目结构和已有测试
5. 生成的测试必须能通过 `gcc -Wall -Werror` 编译
6. 自动运行测试，失败则分析原因并修复（修复策略由你设计）
7. 最终用 gcov 生成覆盖率报告，输出行覆盖率和分支覆盖率
8. 支持 `--coverage-target 80` 参数设定目标覆盖率，未达标时给出建议

### 提示：你会用到哪些模式？

- [ ] **Explore** — 理解项目结构、已有测试、依赖关系
- [ ] **Specialized Agent** — 按你选择的维度拆分测试生成专家
- [ ] **Fan-out / Fan-in** — 并行生成测试（如果你的设计允许并行）
- [ ] **Test-Driven** — 编译 → 运行 → 失败修复循环
- [ ] **Event-Driven** — 可选：Hook 在代码变更后自动重新生成测试

### 参考架构

下面是一种可能的架构（按测试类型拆分）。你完全可以按模块拆、按覆盖率缺口拆、或者混合使用。

```
testgen run <path> [--module parser] [--coverage-target 80]
    │
    ├── 1. Explore: 理解项目结构
    │   └── 输出: 源文件列表、已有测试、公开接口、依赖关系
    │
    ├── 2. 你的 agent 编排（设计由你决定）
    │   ├── 几个 agent？按什么维度拆？
    │   ├── 并行还是串行？
    │   └── 生成的测试文件怎么组织？
    │
    ├── 3. 合并: 统一测试套件
    │   └── 去重、解决冲突、生成 Makefile/CMake 规则
    │
    ├── 4. Test-Driven: 编译运行循环
    │   ├── gcc -Wall -Werror 编译
    │   ├── 运行测试
    │   └── 失败 → 你的修复策略 → 重新编译运行
    │
    └── 5. Coverage: 覆盖率报告
        ├── gcov 生成覆盖率数据
        ├── 对比目标覆盖率
        └── 未达标 → 建议补充测试的函数列表
```

### 验收标准

1. `testgen run ./my-network-lib` 一条命令生成完整测试套件
2. 报告覆盖了你设计的所有测试维度，且能说清拆分理由
3. 生成的测试文件能通过编译且运行结果有通过/失败/跳过的统计
4. 覆盖率报告包含行覆盖率和分支覆盖率
5. 单个 agent 生成失败不影响其他测试的生成

### 进阶挑战

- 支持增量测试生成：只为 git diff 中变更的函数生成新测试
- 集成 ASan（AddressSanitizer）：编译时加 `-fsanitize=address`，捕获运行时内存错误
- 支持 Fuzz Testing：为关键解析函数生成 libFuzzer harness
- 用 Hook 实现"每次修改 .c 文件后自动重跑相关测试"

---

## 作业 3：Security Audit Workflow

> 构建一个自动化安全审计工具，扫描 C/C++ 项目并生成审计报告。

**核心模式**：Explore → Specialized Agent → Fan-out/Fan-in → Sequential

**术语**

- CWE（Common Weakness Enumeration，通用缺陷枚举）
- CVSS（Common Vulnerability Scoring System，通用漏洞评分系统）
- Taint Analysis（污点分析，追踪不可信数据在程序中的传播路径）
- Attack Surface（攻击面，系统中可被攻击者利用的入口集合）
- False Positive（误报，将正常代码错误标记为漏洞）

### 背景

你的团队要对一个 C/C++ 网络库做安全审计。项目包含 HTTP 协议解析器和 TCP 连接池，约 60 个文件，12000 行代码。

和 Review Bot 不同的是，安全审计面临几个独特的挑战：

1. **漏洞有上下文依赖**。同一个 `memcpy()` 调用，如果 size 参数来自硬编码常量就是安全的，如果来自网络输入就可能是 buffer overflow。审计不能只做模式匹配，需要追踪数据流。

2. **误报比漏报更危险**。一份充满误报的审计报告会让开发团队失去信任，之后真正的漏洞也会被忽略。你需要在覆盖面和精度之间找到平衡。

3. **不是所有维度都适合并行**。有些审计发现之间存在关联——比如"输入未校验"和"buffer overflow"往往是同一条攻击链的上下游。盲目拆成独立 agent 可能丢失这种关联。

### 需求描述

**输入**: 一个本地 C/C++ 项目的路径

**输出**: 一份安全审计报告（Markdown 格式），至少包含：
- 审计概览（项目规模、攻击面分析、发现数量）
- 按风险排序的安全问题列表
- 每个问题的攻击场景描述（不只是"这里有 bug"，而是"攻击者可以怎样利用"）
- 整体安全评级

### 你需要自己决定的事

教程中的 Review Bot 把审查拆成了 4 个固定维度（安全/性能/风格/逻辑），这对 code review 是合理的——四个维度之间几乎没有依赖。但安全审计不一定适合这种拆法。

在动手之前，先想清楚这几个设计问题：

**Q1: 按什么维度拆分 agent？**

至少有三种拆法，各有利弊：

- **按漏洞类型拆**（内存安全 / 输入验证 / 资源管理 / ...）：和 Review Bot 最像，容易实现。但会丢失跨类型的攻击链——比如"未校验的网络输入 → 整数溢出 → buffer overflow"横跨了输入验证和内存安全两个 agent。
- **按攻击面拆**（网络入口 / 文件解析 / 配置加载 / ...）：每个 agent 负责一个入口点的端到端审计，能捕捉完整攻击链。但不同入口可能有相同的底层漏洞模式，会重复扫描。
- **按审计阶段拆**（攻击面识别 → 数据流追踪 → 漏洞确认 → 风险评估）：pipeline 模式，前一阶段的输出是后一阶段的输入。精度最高，但变成了串行，失去了并行优势。

你选哪种？还是混合使用？为什么？

**Q2: 怎么处理误报？**

Ch4 中用"清单锚定 + 开放兜底"来平衡精度和覆盖面。但安全审计的误报成本更高——一份 50 条发现中有 30 条误报的报告，没人会认真看。

你打算怎么控制误报率？几个思路供参考：
- 让 agent 对每条发现标注置信度，低置信度的放到附录而非正文
- 增加一个"验证 agent"，专门复审其他 agent 的发现，过滤掉明显的误报
- 在 prompt 中要求 agent 描述攻击场景——如果它说不清"攻击者怎么利用"，大概率是误报

**Q3: 评级怎么定？**

Review Bot 的评级很简单（有 critical 就 FAIL）。但安全审计的评级更复杂：
- 一个需要物理接触才能利用的 critical 漏洞，和一个可远程利用的 warning 漏洞，哪个更严重？
- 评级应该基于漏洞数量，还是基于最严重的单个漏洞，还是基于整体攻击面暴露程度？

你需要设计自己的评级体系，并说明理由。

**技术要求**

1. 用 CLI 封装（typer），支持 `audit run <project-path>` 命令
2. 审计前先用 Explore agent 做攻击面分析（识别网络入口、文件解析入口、外部输入点）
3. agent 的数量和分工由你自己设计——不一定是 5 个，不一定要全部并行
4. 每条发现必须包含：CWE 编号、攻击场景描述、置信度、修复建议
5. 支持 `--output` 参数选择输出格式（markdown / json）
6. 单个 agent 失败不影响整体审计流程

### 提示：你会用到哪些模式？

- [ ] **Sequential** — 审计流程的整体编排
- [ ] **Explore** — 理解目标项目的结构
- [ ] **Specialized Agent** — 按你选择的维度拆分专家
- [ ] **Fan-out / Fan-in** — 并行扫描（如果你的设计允许并行）
- [ ] **Event-Driven** — 可选：Hook 自动触发审计
- [ ] **Test-Driven** — 为审计工具本身写测试

### 参考架构

下面是一种可能的架构（按漏洞类型拆分）。你完全可以设计不同的方案——按攻击面拆、按阶段串行、或者混合使用。

```
audit run <path>
    │
    ├── 1. Explore: 攻击面分析
    │   └── 输出: 入口点清单、数据流入口、外部依赖
    │
    ├── 2. 你的 agent 编排（设计由你决定）
    │   ├── 并行？串行？混合？
    │   ├── 几个 agent？怎么分工？
    │   └── agent 之间需要共享什么上下文？
    │
    ├── 3. 结果收集 + 去重 + 关联分析
    │   └── 同一条攻击链的多个发现是否需要合并？
    │
    └── 4. Report: 聚合 + 评级 + 渲染
```

### 验收标准

1. `audit run ./my-network-lib` 一条命令跑完整个审计
2. 报告覆盖了你设计的所有审计维度，且能说清每个维度的拆分理由
3. 每条发现包含 CWE 编号、攻击场景、置信度、修复建议——缺一不可
4. 报告有你自己设计的评级体系，并在报告中说明评级依据
5. 单个 agent 失败不影响整体审计流程
6. 有基本的单元测试覆盖核心逻辑（聚合、评级、输出格式）

### 进阶挑战

- 支持增量审计：只审计 git diff 中变更的文件，但仍然追踪跨文件的数据流
- 添加 CLAUDE.md 配置，让 Claude Code 自动遵循审计规范
- 用 Hook 实现"每次 push 前自动审计"
- 对比两次审计结果，生成差异报告（新增了哪些漏洞、修复了哪些、哪些恶化了）
- 实现攻击链可视化：把关联的发现串成完整的攻击路径，而不是孤立的点
- 让审计工具审计自己：用你的工具扫描你的工具代码，看看能发现什么

---

## 作业 4：Compliance Test Workflow

> 构建一个自动化合规检测工具，检查终端设备 / 移动操作系统代码是否符合工信部及网络安全法律法规要求。

**核心模式**：Explore → Specialized Agent → Fan-out/Fan-in → Sequential → Event-Driven

**术语**

- 网络安全法（2017 年施行，网络运营者安全保护义务的基本法）
- 个人信息保护法 / PIPL（2021 年施行，个人信息处理规则）
- 数据安全法（2021 年施行，数据处理活动的安全与监管框架）
- 工信部 164 号文（《关于进一步规范移动智能终端应用软件预置行为的通告》，预置 App 可卸载要求）
- GB/T 41391-2022（《移动互联网应用程序（App）收集个人信息基本要求》，最小必要原则）
- 等保 2.0 移动互联安全扩展（GB/T 22239-2019 附录 F，移动终端安全计算环境要求）
- 密码法（2020 年施行，商用密码应用与管理规范）
- 工信部 App 侵害用户权益专项整治（自 2019 年起持续开展，覆盖违规收集信息、强制捆绑、弹窗骚扰等）

### 背景

你的团队负责一款国产移动操作系统的系统级组件开发（C/C++ 实现）。产品即将送检入网，工信部和安全合规团队同时提出要求：系统代码必须通过合规审查，覆盖预置应用管理、个人信息收集、数据存储传输、用户权益保护等多个维度。

手动逐条对照法规太痛苦了——仅 GB/T 41391 就定义了 39 类个人信息的最小必要收集规则，工信部历次专项整治通报涉及的违规类型超过 20 种，等保 2.0 移动互联扩展要求又有数十条技术细则。你决定用 Claude Code 的 workflow 编排，构建一个自动化合规检测工具，按法规维度分类扫描代码，输出合规报告和整改建议。

### 需求描述

**输入**: 一个本地终端设备 / 移动 OS 项目的路径，可选指定法规维度

**输出**:
- 合规检测报告（Markdown 格式）
- 按法规维度分类的违规列表（法规条款、违规位置、风险等级）
- 每条违规的整改建议和代码示例
- 整体合规评级和入网送检风险评估

**检测范围**

合规检测至少需要覆盖以下法规领域（供参考，不是 agent 拆分方案）：

| 法规领域 | 对应法规 | 关注点 |
|---------|---------|--------|
| 个人信息收集 | 个人信息保护法、GB/T 41391-2022 | 最小必要原则、权限与功能匹配、用户同意前置、静默采集设备标识/位置/通讯录 |
| 预置应用与分发 | 工信部 164 号文 | 预置 App 可卸载、第三方应用管理、OTA 推送明示、分发渠道签名校验 |
| 数据存储与传输 | 数据安全法、密码法、等保 2.0 移动扩展 | 敏感数据明文落盘、密钥硬编码、TLS 1.2+、国密算法（SM2/SM3/SM4）、证书校验 |
| 用户权益保护 | 工信部专项整治、网络安全法 | 开屏弹窗强制跳转、强制捆绑下载、频繁自启动/关联启动、欺骗误导点击、通知权限 |
| 终端安全能力 | 等保 2.0 移动互联扩展（附录 F） | 安全启动链、应用沙箱隔离、恶意代码防护、安全审计日志、远程锁定与擦除 |

### 你需要自己决定的事

法规维度是客观存在的，但"法规维度"不等于"agent 架构"。在动手之前，先想清楚这几个设计问题：

**Q1: 法规维度怎么映射到 agent？**

5 个法规领域不一定要用 5 个 agent。几种思路：

- **一对一映射**：每个法规领域一个 agent，最直观。但有些法规之间有交叉——比如"数据存储与传输"和"个人信息收集"都关心敏感数据的处理方式，会重复扫描。
- **按代码区域拆**：每个 agent 负责一组相关模块（如网络层、存储层、权限管理层），在自己的区域内检查所有相关法规。减少重复扫描，但 agent 需要理解多部法规。
- **两阶段混合**：先并行做各法规维度的初筛，再串行做跨维度的关联分析（比如"收集了个人信息但传输时未加密"横跨两个维度）。

**Q2: 评级体系怎么设计？**

合规评级比安全审计更复杂，因为不同违规的法律后果差异巨大：
- 静默收集个人信息可能导致 App 下架甚至行政处罚
- 预置应用不可卸载可能被工信部通报
- 缺少国密算法支持可能影响入网送检但不会被处罚

你的评级应该基于什么？违规数量？法律后果严重程度？入网送检通过概率？还是综合考虑？

**Q3: `--fix` 自动整改的边界在哪？**

有些违规可以自动修复（如补充审计日志字段、替换明文存储为加密存储），有些不能（如重新设计权限申请流程、调整预置应用策略）。你需要决定：
- 哪些类型的违规尝试自动修复，哪些只给建议？
- 自动生成的补丁怎么验证——编译通过就行，还是需要跑测试？
- 修复失败怎么处理——回退并标记，还是保留部分修复？

**技术要求**

1. 用 CLI 封装（typer），支持 `comply check <project-path>` 命令
2. 支持 `--standard` 参数选择法规维度（`pipl` 个保法 / `miit` 工信部规定 / `mlps2` 等保 2.0 / `all` 全部）
3. agent 的数量和分工由你自己设计——不一定和法规维度一一对应
4. 检测前先用 Explore agent 理解项目结构、权限声明文件、系统服务清单、预置应用列表
5. 每条违规标注对应的法规条款（如"PIPL 第 6 条 — 最小必要原则"、"工信部 164 号文 — 预置应用可卸载"）
6. 支持 `--fix` 参数，自动生成整改补丁（整改边界由你设计）
7. 支持 `--output` 参数选择输出格式（markdown / json）

### 提示：你会用到哪些模式？

- [ ] **Sequential** — 合规检测流程的整体编排
- [ ] **Explore** — 理解项目结构、权限声明、系统服务清单、预置应用列表、已有安全措施
- [ ] **Specialized Agent** — 按你选择的维度拆分检测专家
- [ ] **Fan-out / Fan-in** — 并行扫描（如果你的设计允许并行）
- [ ] **Event-Driven** — 可选：Hook 在提交前自动检测合规性
- [ ] **Test-Driven** — 可选：为自动整改生成的补丁跑编译验证

### 参考架构

下面是一种可能的架构（按法规维度一对一映射）。你完全可以按代码区域拆、两阶段混合、或者设计其他方案。

```
comply check <path> [--standard all] [--fix] [--output markdown]
    │
    ├── 1. Explore: 理解项目结构
    │   └── 输出: 源码目录、权限声明文件、预置应用清单、系统服务列表、加密库依赖
    │
    ├── 2. 你的 agent 编排（设计由你决定）
    │   ├── 几个 agent？和法规维度一一对应还是重新组织？
    │   ├── 跨法规的关联违规怎么捕捉？
    │   └── --standard 参数如何影响 agent 的启动？
    │
    ├── 3. 收集 + 关联分析
    │   └── 去重、按法规条款排序、识别跨维度的关联违规
    │
    ├── 4. Report: 聚合 + 评级 + 渲染
    │   └── 输出: 合规报告 + 整改建议 + 入网送检风险评估
    │
    └── 5. Fix（可选）: 自动整改
        ├── 可自动修复的违规 → 生成补丁 → 验证
        └── 不可自动修复的 → 标记为需人工处理
```

### 验收标准

1. `comply check ./my-mobile-os --standard all` 一条命令跑完合规检测
2. 报告覆盖了所有法规领域，且能说清 agent 架构的设计理由
3. 每条违规标注法规条款、文件位置、风险等级
4. 报告包含你自己设计的评级体系和入网送检风险评估
5. 单个 agent 失败不影响其他检测结果

### 进阶挑战

- 支持自定义法规映射：从配置文件加载企业内部合规规则，与国家标准叠加
- 支持增量检测：只检查 git diff 中变更的文件，避免全量扫描
- 对比两次检测结果，生成合规趋势报告（整改前 vs 整改后）
- 用 Hook 实现"每次 commit 前自动检测，存在高危违规则阻止提交"
- 生成工信部送检自查表：按入网检测规范的条款结构输出自查结果，方便提交给检测机构
- 对接 Android CTS / VTS 测试结果，与 AI 检测结果交叉验证
- 支持扫描 APK/HAP 包：解析已编译的应用包，检测权限声明与实际行为是否一致

