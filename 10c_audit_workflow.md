# 作业 3：Security Audit Workflow

> 构建一个自动化安全审计工具，扫描 C/C++ 项目并生成审计报告。

**核心模式**：Explore → Specialized Agent → Fan-out/Fan-in → Sequential

**术语**

- CWE（Common Weakness Enumeration，通用缺陷枚举）
- CVSS（Common Vulnerability Scoring System，通用漏洞评分系统）
- Taint Analysis（污点分析，追踪不可信数据在程序中的传播路径）
- Attack Surface（攻击面，系统中可被攻击者利用的入口集合）
- False Positive（误报，将正常代码错误标记为漏洞）

---

## 背景

你的团队要对一个 C/C++ 网络库做安全审计。项目包含 HTTP 协议解析器和 TCP 连接池，约 60 个文件，12000 行代码。

和 Review Bot 不同的是，安全审计面临几个独特的挑战：

1. **漏洞有上下文依赖**。同一个 `memcpy()` 调用，如果 size 参数来自硬编码常量就是安全的，如果来自网络输入就可能是 buffer overflow。审计不能只做模式匹配，需要追踪数据流。

2. **误报比漏报更危险**。一份充满误报的审计报告会让开发团队失去信任，之后真正的漏洞也会被忽略。你需要在覆盖面和精度之间找到平衡。

3. **不是所有维度都适合并行**。有些审计发现之间存在关联——比如"输入未校验"和"buffer overflow"往往是同一条攻击链的上下游。盲目拆成独立 agent 可能丢失这种关联。

---

## 需求描述

### 功能需求

**输入**: 一个本地 C/C++ 项目的路径

**输出**: 一份安全审计报告（Markdown 格式），至少包含：
- 审计概览（项目规模、攻击面分析、发现数量）
- 按风险排序的安全问题列表
- 每个问题的攻击场景描述（不只是"这里有 bug"，而是"攻击者可以怎样利用"）
- 整体安全评级

### 你需要自己决定的事

教程中的 Review Bot 把审查拆成了 4 个固定维度（安全/性能/风格/逻辑），这对 code review 是合理的——四个维度之间几乎没有依赖。但安全审计不一定适合这种拆法。

在动手之前，先想清楚这几个设计问题：

**Q1: 按什么维度拆分 agent？**

至少有三种拆法，各有利弊：

- **按漏洞类型拆**（内存安全 / 输入验证 / 资源管理 / ...）：和 Review Bot 最像，容易实现。但会丢失跨类型的攻击链——比如"未校验的网络输入 → 整数溢出 → buffer overflow"横跨了输入验证和内存安全两个 agent。
- **按攻击面拆**（网络入口 / 文件解析 / 配置加载 / ...）：每个 agent 负责一个入口点的端到端审计，能捕捉完整攻击链。但不同入口可能有相同的底层漏洞模式，会重复扫描。
- **按审计阶段拆**（攻击面识别 → 数据流追踪 → 漏洞确认 → 风险评估）：pipeline 模式，前一阶段的输出是后一阶段的输入。精度最高，但变成了串行，失去了并行优势。

你选哪种？还是混合使用？为什么？

**Q2: 怎么处理误报？**

Ch4 中我们用"清单锚定 + 开放兜底"来平衡精度和覆盖面。但安全审计的误报成本更高——一份 50 条发现中有 30 条误报的报告，没人会认真看。

你打算怎么控制误报率？几个思路供参考：
- 让 agent 对每条发现标注置信度，低置信度的放到附录而非正文
- 增加一个"验证 agent"，专门复审其他 agent 的发现，过滤掉明显的误报
- 在 prompt 中要求 agent 描述攻击场景——如果它说不清"攻击者怎么利用"，大概率是误报

**Q3: 评级怎么定？**

Review Bot 的评级很简单（有 critical 就 FAIL）。但安全审计的评级更复杂：
- 一个需要物理接触才能利用的 critical 漏洞，和一个可远程利用的 warning 漏洞，哪个更严重？
- 评级应该基于漏洞数量，还是基于最严重的单个漏洞，还是基于整体攻击面暴露程度？

你需要设计自己的评级体系，并说明理由。

### 技术要求

1. 用 CLI 封装（typer），支持 `audit run <project-path>` 命令
2. 审计前先用 Explore agent 做攻击面分析（识别网络入口、文件解析入口、外部输入点）
3. agent 的数量和分工由你自己设计——不一定是 5 个，不一定要全部并行
4. 每条发现必须包含：CWE 编号、攻击场景描述、置信度、修复建议
5. 支持 `--output` 参数选择输出格式（markdown / json）
6. 单个 agent 失败不影响整体审计流程

---

## 提示：你会用到哪些模式？

- [ ] **Sequential** — 审计流程的整体编排
- [ ] **Explore** — 理解目标项目的结构
- [ ] **Specialized Agent** — 按你选择的维度拆分专家
- [ ] **Fan-out / Fan-in** — 并行扫描（如果你的设计允许并行）
- [ ] **Event-Driven** — 可选：Hook 自动触发审计
- [ ] **Test-Driven** — 为审计工具本身写测试

---

## 参考架构

下面是一种可能的架构（按漏洞类型拆分）。你完全可以设计不同的方案——按攻击面拆、按阶段串行、或者混合使用。

```
audit run <path>
    │
    ├── 1. Explore: 攻击面分析
    │   └── 输出: 入口点清单、数据流入口、外部依赖
    │
    ├── 2. 你的 agent 编排（设计由你决定）
    │   ├── 并行？串行？混合？
    │   ├── 几个 agent？怎么分工？
    │   └── agent 之间需要共享什么上下文？
    │
    ├── 3. 结果收集 + 去重 + 关联分析
    │   └── 同一条攻击链的多个发现是否需要合并？
    │
    └── 4. Report: 聚合 + 评级 + 渲染
```

---

## 验收标准

1. `audit run ./my-network-lib` 一条命令跑完整个审计
2. 报告覆盖了你设计的所有审计维度，且能说清每个维度的拆分理由
3. 每条发现包含 CWE 编号、攻击场景、置信度、修复建议——缺一不可
4. 报告有你自己设计的评级体系，并在报告中说明评级依据
5. 单个 agent 失败不影响整体审计流程
6. 有基本的单元测试覆盖核心逻辑（聚合、评级、输出格式）

---

## 进阶挑战

- 支持增量审计：只审计 git diff 中变更的文件，但仍然追踪跨文件的数据流
- 添加 CLAUDE.md 配置，让 Claude Code 自动遵循审计规范
- 用 Hook 实现"每次 push 前自动审计"
- 对比两次审计结果，生成差异报告（新增了哪些漏洞、修复了哪些、哪些恶化了）
- 实现攻击链可视化：把关联的发现串成完整的攻击路径，而不是孤立的点
- 让审计工具审计自己：用你的工具扫描你的工具代码，看看能发现什么
